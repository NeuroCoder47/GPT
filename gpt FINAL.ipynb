{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F # type: ignore\n",
    "from torch.distributions import Categorical\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2, EnWik9, AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Here is an extract from a webpage: \"What can cause my settlement offer to be delayed?\\nWhen you’ve been injured in an Austin truck accident, one of the most common questions is how long it will take for the insurance company to make an offer to settle your case. The answer depends on a variety of factors.\\nThe process starts with filing an insurance claim and providing evidence that shows exactly what happened during the accident and who was at fault. This can involve gathering key Austin truck accident evidence such as:\\n- Medical records\\n- Photographs or video footage of the crash scene\\n- Witness statements\\n- Other documents related to your injuries and damages.\\nOnce this information has been collected by both sides, negotiations may begin between your Austin truck accident lawyer and the insurance company on how much compensation should be offered in exchange for settling the case out of court.\\nIt is important to remember that every truck accident case is unique so there is no set timeline as far as when a settlement mig\".\\n\\nWrite an informative and insightful blog post that expands upon the extract above. Your post should delve into the nuances of the topic, offering fresh perspectives and deeper analysis. Aim to:\\n\\n- Inform: Provide valuable, well-researched information that educates the reader.\\n- Engage: Write in a conversational tone that connects with the audience, making complex ideas accessible.\\n- Illustrate: Use examples, anecdotes, or personal experiences to bring the topic to life.\\nDo not give a title and do not start with sentences like \"Have you ever...\" or \"Hello dear readers..\", simply write the content without these introductory phrases.', 'text_token_length': 585, 'text': \" When you've been involved in an auto accident, particularly one involving a commercial truck, receiving a settlement offer from the insurance company is often top of mind. After all, medical bills, lost wages, and property damage can quickly add up, leaving you financially strained. However, the timing of a settlement offer can vary greatly depending on several factors. Let's delve deeper into the nuances of this topic.\\n\\nFirst and foremost, before any settlement negotiation can occur, it's crucial to establish liability. Gathering evidence such as medical records, photographs, witness statements, and other relevant documentation helps build a solid foundation for your case. An experienced Austin truck accident attorney can guide you through this process and ensure that all necessary evidence is gathered and presented effectively. \\n\\nOne factor that can significantly impact the timeline of a settlement offer is the complexity of the case itself. For instance, if multiple parties are involved, determining responsibility becomes more intricate, potentially delaying the settlement process. Additionally, cases involving severe injuries usually require extensive medical evaluations and treatment plans, which takes time to compile and present accurately.\\n\\nAnother critical aspect influencing the speed of a settlement offer is communication between the two negotiating parties – i.e., your legal representative and the insurance adjuster. While some adjusters work diligently to resolve claims swiftly, others might employ stall tactics designed to lowball offers or wear down claimants. Patience and perseverance are essential here; attempting to rush the process could result in a lower payout than what you rightfully deserve.\\n\\nFurthermore, keep in mind that each case is unique, meaning there's no standard timeline for receiving a settlement offer following a truck accident. Some claims may reach resolution within months, while others might drag on for over a year due to unforeseen complications or disputes. It's vital to stay informed throughout the process, maintaining open lines of communication with your attorney to understand where things stand and what to expect moving forward.\\n\\nLastly, consider mediation as a viable option if negotiations become tense or protracted. Mediators act as impartial third parties who facilitate discussions between opposing counsel, aiming to find middle ground and expedite resolutions. By bringing in a mediator, both sides agree to compromise, often leading to faster (and fairer) outcomes.\\n\\nIn conclusion, various elements contribute to the length of time it takes for a settlement offer after an Austin truck accident. Building a strong case through thorough evidence collection, exercising patience during negotiations, and considering alternative dispute resolution methods can help streamline the process. Remember, staying informed and working closely with an experienced truck accident lawyer increases your chances of securing a favorable outcome, even if it requires a bit of waiting.\", 'seed_data': 'web_samples_v2', 'format': 'blogpost', 'audience': 'general'}\n",
      "Dataset successfully saved to CSV.\n",
      "Data successfully split and saved to train_data.csv and test_data.csv.\n",
      "Files successfully combined into combined_data.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"HuggingFaceTB/cosmopedia-100k\", cache_dir=r\"C:\\Users\\ashmi\\Downloads\")\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the first sample from the train set\n",
    "print(ds['train'][0])\n",
    "\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "df = pd.DataFrame(ds['train'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(r\"C:\\Users\\ashmi\\Downloads\\cosmopedia_100k_train.csv\", index=False)\n",
    "print(\"Dataset successfully saved to CSV.\")\n",
    "\n",
    "# Load the saved CSV file\n",
    "df = pd.read_csv(r\"C:\\Users\\ashmi\\Downloads\\cosmopedia_100k_train.csv\")\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['seed_data', 'format', 'audience', 'text_token_length', 'prompt'])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to clean the text in the DataFrame\n",
    "def clean_text(df):\n",
    "    df['text'] = df['text'].apply(lambda text: re.sub(r'\\|\\n|;', ' ', text.replace('\"', ' ').replace('\\n', ' ')).lower())\n",
    "    return df\n",
    "\n",
    "# Clean the text in train and test sets\n",
    "train_df = clean_text(train_df)\n",
    "test_df = clean_text(test_df)\n",
    "\n",
    "# Define the file paths for the train and test CSV files\n",
    "data_set_root = r\"C:\\Users\\ashmi\\Downloads\"\n",
    "train_csv_path = os.path.join(data_set_root, \"train_data.csv\")\n",
    "test_csv_path = os.path.join(data_set_root, \"test_data.csv\")\n",
    "\n",
    "# Save the train and test sets as CSV files\n",
    "train_df.to_csv(train_csv_path, index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(test_csv_path, index=False, encoding=\"utf-8\")\n",
    "print(\"Data successfully split and saved to train_data.csv and test_data.csv.\")\n",
    "\n",
    "# Combine the train and test CSV files into one\n",
    "combined_txt_path = os.path.join(data_set_root, \"combined_data.txt\")\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Save the combined data as a txt file\n",
    "combined_df.to_csv(combined_txt_path, index=False, sep='\\t', encoding=\"utf-8\")\n",
    "print(\"Files successfully combined into combined_data.txt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader objects created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df['text'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "dataset_train = TextDataset(train_df)\n",
    "dataset_test = TextDataset(test_df)\n",
    "\n",
    "\n",
    "shuffle = True\n",
    "\n",
    "# Create DataLoader objects\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=shuffle)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader objects created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=r\"C:\\Users\\ashmi\\Downloads/combined_data.txt\",  # specify the correct path\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"tok400\",  # output filename prefix\n",
    "  # algorithm spec\n",
    "  model_type=\"bpe\",  # BPE algorithm\n",
    "  vocab_size=8000,  # increased from 361 to 8000 to accommodate required characters and allow for common subwords\n",
    "  # normalization\n",
    "  normalization_rule_name=\"identity\",  # turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=100059,  # max number of training sentences\n",
    "  max_sentence_length=4192,  # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.9995,  # slightly reduced from 0.99995\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0,  # the UNK token MUST exist\n",
    "  bos_id=1,  # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(),  # use ~all system resources\n",
    ")\n",
    "\n",
    "# Train the SentencePiece model\n",
    "spm.SentencePieceTrainer.train(**options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: hi how are you\n",
      "Tokenized Sentence: ['▁hi', '▁how', '▁are', '▁you']\n",
      "Token IDs: [5302, 472, 527, 352]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load the trained SentencePiece model\n",
    "model_path = \"tok400.model\"  # Update the path if necessary\n",
    "sp = spm.SentencePieceProcessor(model_file=r\"tok400.model\")\n",
    "\n",
    "def tokenize_and_convert(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = sp.encode(sentence, out_type=str)  # Tokenize to string tokens\n",
    "    \n",
    "    # Convert tokens to IDs\n",
    "    token_ids = sp.encode(sentence, out_type=int)  # Convert to integer IDs\n",
    "    \n",
    "    return tokens, token_ids\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"hi how are you\"\n",
    "# Tokenize and convert the sentence\n",
    "tokens, token_ids = tokenize_and_convert(sentence)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Tokenized Sentence:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\"For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n",
    "    \n",
    "    Args:\n",
    "        prob (float): probability of dropping a token\n",
    "        pad_token (int): index for the <pad> token\n",
    "        num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p=prob\n",
    "        # to create a mask where 1 means we will replace that token\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        \n",
    "        # only replace if the token is not a special token\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        mask = mask * can_drop\n",
    "        \n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        \n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        \n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.transforms import SentencePieceTokenizer, ToTensor\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Load SentencePiece model\n",
    "sp = spm.SentencePieceProcessor(model_file=\"tok400.model\")\n",
    "\n",
    "# Tokenization and conversion to IDs\n",
    "def tokenize_and_convert(text):\n",
    "    return sp.encode(text, out_type=int)\n",
    "\n",
    "# Padding\n",
    "def pad_sequence(token_ids, max_len=128, padding_value=0):\n",
    "    if len(token_ids) < max_len:\n",
    "        return token_ids + [padding_value] * (max_len - len(token_ids))\n",
    "    else:\n",
    "        return token_ids[:max_len]\n",
    "\n",
    "# Custom function to apply tokenization, padding, and conversion to tensor\n",
    "def transform(text, max_len=128, padding_value=0):\n",
    "    token_ids = tokenize_and_convert(text)\n",
    "    padded_token_ids = pad_sequence(token_ids, max_len=max_len, padding_value=padding_value)\n",
    "    return torch.tensor(padded_token_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader objects created successfully.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define the dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        if self.transform:\n",
    "            text = self.transform(text)\n",
    "        return text\n",
    "\n",
    "# Create dataset objects with transformations\n",
    "conv_dataset_train = TextDataset(train_df, transform=lambda x: transform(x, max_len=128, padding_value=0))\n",
    "conv_dataset_test = TextDataset(test_df, transform=lambda x: transform(x, max_len=128, padding_value=0))\n",
    "\n",
    "\n",
    "shuffle = True\n",
    "\n",
    "# Create DataLoader objects\n",
    "conv_data_loader_train = DataLoader(conv_dataset_train, batch_size=batch_size, shuffle=shuffle)\n",
    "conv_data_loader_test = DataLoader(conv_dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"DataLoader objects created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting Training DataLoader:\n",
      "Vocabulary size: 8000\n",
      "\n",
      "Batch 1:\n",
      "Batch shape: torch.Size([1000, 128])\n",
      "Data type: torch.int64\n",
      "Device: cpu\n",
      "Min token ID: 0\n",
      "Max token ID: 7999\n",
      "Sample sequence: [375, 7938, 859, 345, 276, 354, 6101, 449, 261, 4947]...\n",
      "Decoded sample:  i work as the it manager at a medium-sized financ...\n",
      "\n",
      "Batch 2:\n",
      "Batch shape: torch.Size([1000, 128])\n",
      "Data type: torch.int64\n",
      "Device: cpu\n",
      "Min token ID: 12\n",
      "Max token ID: 7999\n",
      "Sample sequence: [7935, 1100, 7966, 472, 296, 3073, 296, 261, 896, 6204]...\n",
      "Decoded sample:  title: how to respond to a potential domestic vio...\n",
      "\n",
      "Batch 3:\n",
      "Batch shape: torch.Size([1000, 128])\n",
      "Data type: torch.int64\n",
      "Device: cpu\n",
      "Min token ID: 12\n",
      "Max token ID: 7999\n",
      "Sample sequence: [7935, 6472, 2631, 401, 261, 3744, 345, 2006, 345, 4717]...\n",
      "Decoded sample:  breadmaking is a craft as old as civilization its...\n",
      "\n",
      "Batch 4:\n",
      "Batch shape: torch.Size([1000, 128])\n",
      "Data type: torch.int64\n",
      "Device: cpu\n",
      "Min token ID: 0\n",
      "Max token ID: 7999\n",
      "Sample sequence: [7935, 261, 6406, 3243, 4325, 7957, 521, 1110, 3284, 7331]...\n",
      "Decoded sample:  a couple months ago, i found myself scrolling thr...\n",
      "\n",
      "Batch 5:\n",
      "Batch shape: torch.Size([1000, 128])\n",
      "Data type: torch.int64\n",
      "Device: cpu\n",
      "Min token ID: 11\n",
      "Max token ID: 7999\n",
      "Sample sequence: [7935, 1100, 7966, 261, 5755, 3660, 285, 2737, 7955, 374]...\n",
      "Decoded sample:  title: a surprising discovery in denbigh's job ma...\n",
      "\n",
      "DataLoader inspection complete.\n",
      "\n",
      "Inspecting Test DataLoader:\n",
      "Vocabulary size: 8000\n",
      "\n",
      "Batch 1:\n",
      "Batch shape: torch.Size([1000, 128])\n",
      "Data type: torch.int64\n",
      "Device: cpu\n",
      "Min token ID: 0\n",
      "Max token ID: 7999\n",
      "Sample sequence: [7935, 1100, 7966, 276, 874, 298, 2014, 7966, 3112, 648]...\n",
      "Decoded sample:  title: the power of connection: supporting transg...\n",
      "\n",
      "Batch 2:\n",
      "Batch shape: torch.Size([1000, 128])\n",
      "Data type: torch.int64\n",
      "Device: cpu\n",
      "Min token ID: 12\n",
      "Max token ID: 7999\n",
      "Sample sequence: [7935, 4306, 1100, 7966, 1558, 261, 6465, 1218, 7966, 1903]...\n",
      "Decoded sample:  chapter title: building a winning team: lessons f...\n",
      "\n",
      "Batch 3:\n",
      "Batch shape: torch.Size([1000, 128])\n",
      "Data type: torch.int64\n",
      "Device: cpu\n",
      "Min token ID: 12\n",
      "Max token ID: 7999\n",
      "Sample sequence: [375, 7975, 7958, 7969, 2639, 298, 2880, 288, 1255, 1353]...\n",
      "Decoded sample:  3.1 overview of iranian art and literature  the r...\n",
      "\n",
      "Batch 4:\n",
      "Batch shape: torch.Size([1000, 128])\n",
      "Data type: torch.int64\n",
      "Device: cpu\n",
      "Min token ID: 0\n",
      "Max token ID: 7999\n",
      "Sample sequence: [7935, 354, 7962, 7942, 710, 4544, 380, 992, 5210, 1357]...\n",
      "Decoded sample:  it's no secret that many uk homeowners are hesita...\n",
      "\n",
      "Batch 5:\n",
      "Batch shape: torch.Size([1000, 128])\n",
      "Data type: torch.int64\n",
      "Device: cpu\n",
      "Min token ID: 12\n",
      "Max token ID: 7999\n",
      "Sample sequence: [7935, 1100, 7966, 472, 296, 4769, 4354, 317, 310, 4746]...\n",
      "Decoded sample:  title: how to handle intimidation tactics on the ...\n",
      "\n",
      "DataLoader inspection complete.\n"
     ]
    }
   ],
   "source": [
    "def inspect_dataloader(dataloader, num_batches=5):\n",
    "    vocab_size = sp.GetPieceSize()\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        \n",
    "        print(f\"\\nBatch {i+1}:\")\n",
    "        print(f\"Batch shape: {batch.shape}\")\n",
    "        print(f\"Data type: {batch.dtype}\")\n",
    "        print(f\"Device: {batch.device}\")\n",
    "        print(f\"Min token ID: {batch.min().item()}\")\n",
    "        print(f\"Max token ID: {batch.max().item()}\")\n",
    "        \n",
    "        # Check for out-of-range token IDs\n",
    "        out_of_range = (batch >= vocab_size).sum().item()\n",
    "        if out_of_range > 0:\n",
    "            print(f\"Warning: {out_of_range} token IDs are out of vocabulary range\")\n",
    "        \n",
    "        # Print a sample sequence from the batch\n",
    "        sample_seq = batch[0].tolist()  # Convert the first sequence in the batch to a list\n",
    "        print(f\"Sample sequence: {sample_seq[:10]}...\")  # Print first 10 tokens\n",
    "        \n",
    "        # Decode a sample sequence\n",
    "        decoded_seq = sp.DecodeIds(sample_seq)\n",
    "        print(f\"Decoded sample: {decoded_seq[:50]}...\")  # Print first 50 characters\n",
    "        \n",
    "    print(\"\\nDataLoader inspection complete.\")\n",
    "\n",
    "# Inspect the training DataLoader\n",
    "print(\"Inspecting Training DataLoader:\")\n",
    "inspect_dataloader(conv_data_loader_train)\n",
    "\n",
    "# Inspect the test DataLoader\n",
    "print(\"\\nInspecting Test DataLoader:\")\n",
    "inspect_dataloader(conv_data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1000, 128])\n"
     ]
    }
   ],
   "source": [
    "for batch in conv_data_loader_train:\n",
    "    print(type(batch))  # Should be <class 'torch.Tensor'>\n",
    "    print(batch.shape)  # Check the shape of the tensor\n",
    "    break  # Remove this if you want to inspect all batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "# Learning rate for the optimizer\n",
    "\n",
    "# Batch size for data loaders\n",
    "batch_size = 256\n",
    "\n",
    "# Maximum sequence length for text inputs\n",
    "max_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal positional embeddings\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional embeddings module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate sinusoidal positional embeddings\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "    \n",
    "# Transformer block with Attention and causal masking\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with self-attention and causal masking.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=128, num_heads=4):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # Layer normalization for input\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Multi-head self-attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, \n",
    "                                                    num_heads=num_heads, \n",
    "                                                    batch_first=True,\n",
    "                                                    dropout=0.1)\n",
    "\n",
    "        # Layer normalization for attention output\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Feedforward neural network\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, padding_mask):\n",
    "        # Create causal mask for Attention\n",
    "        bs, l, h = x.shape\n",
    "        mask = torch.triu(torch.ones(l, l, device=x.device), 1).bool()\n",
    "\n",
    "        # Layer normalization\n",
    "        norm_x = self.norm1(x)\n",
    "\n",
    "        # Apply multi-head Attention\n",
    "        x = self.multihead_attn(norm_x, norm_x, norm_x, attn_mask=mask, key_padding_mask=padding_mask)[0] + x\n",
    "\n",
    "        # Layer normalization\n",
    "        norm_x = self.norm2(x)\n",
    "\n",
    "        # Apply feedforward neural network\n",
    "        x = self.mlp(norm_x) + x\n",
    "        return x\n",
    "\n",
    "    \n",
    "# \"Decoder-Only\" Style Transformer with Attention\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    \"Decoder-Only\" Style Transformer with self-attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Token embeddings\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        # List of Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Mask for padding tokens\n",
    "        input_key_mask = input_seq == 0\n",
    "\n",
    "        # Embedding input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to token embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "\n",
    "        # Pass through Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs, padding_mask=input_key_mask)\n",
    "\n",
    "        # Output predictions\n",
    "        return self.fc_out(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emb=sp.GetPieceSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Load the SentencePiece model\n",
    "sp = spm.SentencePieceProcessor(model_file=r\"tok400.model\")\n",
    "\n",
    "# Check if GPU is available, set device accordingly\n",
    "# Embedding Size\n",
    "hidden_size = 256\n",
    "\n",
    "# Number of transformer blocks\n",
    "num_layers =8\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8\n",
    "\n",
    "# Create model\n",
    "tf_generator = Transformer(num_emb=sp.GetPieceSize(), num_layers=num_layers, \n",
    "                           hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(tf_generator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "td = TokenDrop(prob=0.2, pad_token=sp.pad_id())\n",
    "\n",
    "# Initialize training loss logger and entropy logger\n",
    "training_loss_logger = []\n",
    "entropy_logger = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-This Model Has 10422080 (Approximately 10 Million) Parameters!\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "num_model_params = count_parameters(tf_generator)\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params/1000000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torch.distributed as dist\\nfrom torch.nn.parallel import DistributedDataParallel as DDP\\nfrom torch.cuda.amp import GradScaler, autocast\\nfrom tqdm import tqdm\\nimport os\\nimport math\\nimport time\\n\\n# Initialize distributed process group if running in DDP mode\\nddp = int(os.environ.get(\\'RANK\\', -1)) != -1\\nif ddp:\\n    assert torch.cuda.is_available(), \"CUDA is required for DDP.\"\\n    dist.init_process_group(backend=\\'nccl\\')\\n    ddp_rank = int(os.environ[\\'RANK\\'])\\n    ddp_local_rank = int(os.environ[\\'LOCAL_RANK\\'])\\n    ddp_world_size = int(os.environ[\\'WORLD_SIZE\\'])\\n    device = f\\'cuda:{ddp_local_rank}\\'\\n    torch.cuda.set_device(device)\\n    master_process = (ddp_rank == 0)\\nelse:\\n    ddp_rank = 0\\n    ddp_local_rank = 0\\n    ddp_world_size = 1\\n    master_process = True\\n    device = \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'\\n\\n# Set device type for mixed precision training\\ndevice_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\\n\\ntorch.manual_seed(1337)\\nif torch.cuda.is_available():\\n    torch.cuda.manual_seed(1337)\\n\\n# Model initialization (replace tf_generator with actual model)\\nmodel = tf_generator  # Assume your model is defined as `tf_generator`\\nmodel.to(device)\\n\\n# Enable distributed training\\nif ddp:\\n    model = DDP(model, device_ids=[ddp_local_rank])\\nraw_model = model.module if ddp else model\\n\\n# Optimizer with AdamW and weight decay\\noptimizer = optim.AdamW(raw_model.parameters(), lr=1e-4, weight_decay=0.1)\\n\\n# Scaler for mixed precision training\\nscaler = GradScaler()\\n\\n# Loss function\\nloss_fn = nn.CrossEntropyLoss(reduction=\\'mean\\')\\n\\n# Learning rate scheduler parameters\\nmax_lr = 1e-4\\nmin_lr = max_lr * 0.1\\nwarmup_steps = 500  # Example warm-up steps\\nmax_steps = 20000  # Total iterations (same as max_iters in original code)\\n\\ndef get_lr(step):\\n    if step < warmup_steps:\\n        return max_lr * (step + 1) / warmup_steps\\n    if step > max_steps:\\n        return min_lr\\n    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\\n    return min_lr + coeff * (max_lr - min_lr)\\n\\n# Gradient clipping threshold\\ngrad_clip = 1.0\\n\\n# Custom logging and checkpoint directory\\nlog_dir = \"log\"\\nos.makedirs(log_dir, exist_ok=True)\\nlog_file = os.path.join(log_dir, \"log.txt\")\\nwith open(log_file, \"w\") as f:\\n    pass  # Clear the log file\\n\\n# Training loop using iterations\\ntrain_loader = conv_data_loader_train  # Assuming this is the dataloader\\nval_loader = conv_data_loader_test     # Assuming this is the validation dataloader\\niteration = 0\\ntotal_loss = 0\\nprogress_bar = tqdm(total=max_steps, desc=\"Training\", leave=False)\\n\\nwhile iteration < max_steps:\\n    for batch in train_loader:\\n        if iteration >= max_steps:\\n            break\\n        \\n        # Move batch to device\\n        batch = batch.to(device)\\n\\n        # Forward pass with autocast for mixed precisio\\n        # nn\\n        # Forward pass with autocast for mixed precision\\n        with autocast(dtype=torch.bfloat16):\\n            output = model(batch)\\n            loss = loss_fn(output.view(-1, sp.GetPieceSize()), batch.view(-1))\\n\\n        # Backward pass with gradient scaling\\n        optimizer.zero_grad()\\n        scaler.scale(loss).backward()\\n        \\n        # Clip gradients\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\\n\\n        # Update optimizer with scaler\\n        scaler.step(optimizer)\\n        scaler.update()\\n\\n        # Update learning rate\\n        lr = get_lr(iteration)\\n        for param_group in optimizer.param_groups:\\n            param_group[\\'lr\\'] = lr\\n\\n        # Log the training loss\\n        total_loss += loss.item()\\n        if iteration % 100 == 0:\\n            avg_loss = total_loss / 100\\n            if master_process:\\n                print(f\"Iteration {iteration}/{max_steps}, Training Loss: {avg_loss:.4f}, LR: {lr:.6f}\")\\n                with open(log_file, \"a\") as f:\\n                    f.write(f\"{iteration} train {avg_loss:.6f}\\n\")\\n            total_loss = 0\\n\\n            # Validation loop\\n            model.eval()\\n            val_loss = 0\\n            val_batches = 0\\n            with torch.no_grad():\\n                for val_batch in val_loader:\\n                    val_batch = val_batch.to(device)\\n                    with autocast(dtype=torch.bfloat16):\\n                        val_output = model(val_batch)\\n                        val_loss += loss_fn(val_output.view(-1, sp.GetPieceSize()), val_batch.view(-1)).item()\\n                    val_batches += 1\\n            avg_val_loss = val_loss / val_batches\\n            if master_process:\\n                print(f\"Iteration {iteration}/{max_steps}, Validation Loss: {avg_val_loss:.4f}\")\\n                with open(log_file, \"a\") as f:\\n                    f.write(f\"{iteration} val {avg_val_loss:.6f}\\n\")\\n            model.train()\\n\\n        iteration += 1\\n        progress_bar.update(1)\\n\\nprogress_bar.close()\\n\\nif ddp:\\n    dist.destroy_process_group()\\n\\nprint(\"Training complete!\")'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Initialize distributed process group if running in DDP mode\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), \"CUDA is required for DDP.\"\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = (ddp_rank == 0)\n",
    "else:\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set device type for mixed precision training\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# Model initialization (replace tf_generator with actual model)\n",
    "model = tf_generator  # Assume your model is defined as `tf_generator`\n",
    "model.to(device)\n",
    "\n",
    "# Enable distributed training\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "raw_model = model.module if ddp else model\n",
    "\n",
    "# Optimizer with AdamW and weight decay\n",
    "optimizer = optim.AdamW(raw_model.parameters(), lr=1e-4, weight_decay=0.1)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Learning rate scheduler parameters\n",
    "max_lr = 1e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 500  # Example warm-up steps\n",
    "max_steps = 20000  # Total iterations (same as max_iters in original code)\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step + 1) / warmup_steps\n",
    "    if step > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "# Gradient clipping threshold\n",
    "grad_clip = 1.0\n",
    "\n",
    "# Custom logging and checkpoint directory\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, \"log.txt\")\n",
    "with open(log_file, \"w\") as f:\n",
    "    pass  # Clear the log file\n",
    "\n",
    "# Training loop using iterations\n",
    "train_loader = conv_data_loader_train  # Assuming this is the dataloader\n",
    "val_loader = conv_data_loader_test     # Assuming this is the validation dataloader\n",
    "iteration = 0\n",
    "total_loss = 0\n",
    "progress_bar = tqdm(total=max_steps, desc=\"Training\", leave=False)\n",
    "\n",
    "while iteration < max_steps:\n",
    "    for batch in train_loader:\n",
    "        if iteration >= max_steps:\n",
    "            break\n",
    "        \n",
    "        # Move batch to device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass with autocast for mixed precisio\n",
    "        # nn\n",
    "        # Forward pass with autocast for mixed precision\n",
    "        with autocast(dtype=torch.bfloat16):\n",
    "            output = model(batch)\n",
    "            loss = loss_fn(output.view(-1, sp.GetPieceSize()), batch.view(-1))\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        # Update optimizer with scaler\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Update learning rate\n",
    "        lr = get_lr(iteration)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Log the training loss\n",
    "        total_loss += loss.item()\n",
    "        if iteration % 100 == 0:\n",
    "            avg_loss = total_loss / 100\n",
    "            if master_process:\n",
    "                print(f\"Iteration {iteration}/{max_steps}, Training Loss: {avg_loss:.4f}, LR: {lr:.6f}\")\n",
    "                with open(log_file, \"a\") as f:\n",
    "                    f.write(f\"{iteration} train {avg_loss:.6f}\\n\")\n",
    "            total_loss = 0\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    val_batch = val_batch.to(device)\n",
    "                    with autocast(dtype=torch.bfloat16):\n",
    "                        val_output = model(val_batch)\n",
    "                        val_loss += loss_fn(val_output.view(-1, sp.GetPieceSize()), val_batch.view(-1)).item()\n",
    "                    val_batches += 1\n",
    "            avg_val_loss = val_loss / val_batches\n",
    "            if master_process:\n",
    "                print(f\"Iteration {iteration}/{max_steps}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "                with open(log_file, \"a\") as f:\n",
    "                    f.write(f\"{iteration} val {avg_val_loss:.6f}\\n\")\n",
    "            model.train()\n",
    "\n",
    "        iteration += 1\n",
    "        progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "if ddp:\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "print(\"Training complete!\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:02<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 6.6771\n",
      "Model saved at epoch 1\n",
      "Epoch 2/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 6.0461\n",
      "Model saved at epoch 2\n",
      "Epoch 3/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 5.6495\n",
      "Model saved at epoch 3\n",
      "Epoch 4/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 5.3726\n",
      "Model saved at epoch 4\n",
      "Epoch 5/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 5.1812\n",
      "Model saved at epoch 5\n",
      "Epoch 6/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 5.0365\n",
      "Model saved at epoch 6\n",
      "Epoch 7/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.9177\n",
      "Model saved at epoch 7\n",
      "Epoch 8/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.8179\n",
      "Model saved at epoch 8\n",
      "Epoch 9/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.7309\n",
      "Model saved at epoch 9\n",
      "Epoch 10/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.6554\n",
      "Model saved at epoch 10\n",
      "Epoch 11/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.5868\n",
      "Model saved at epoch 11\n",
      "Epoch 12/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.5261\n",
      "Model saved at epoch 12\n",
      "Epoch 13/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.4709\n",
      "Model saved at epoch 13\n",
      "Epoch 14/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.4202\n",
      "Model saved at epoch 14\n",
      "Epoch 15/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.3742\n",
      "Model saved at epoch 15\n",
      "Epoch 16/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.3306\n",
      "Model saved at epoch 16\n",
      "Epoch 17/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.2910\n",
      "Model saved at epoch 17\n",
      "Epoch 18/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.2536\n",
      "Model saved at epoch 18\n",
      "Epoch 19/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.2184\n",
      "Model saved at epoch 19\n",
      "Epoch 20/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.1854\n",
      "Model saved at epoch 20\n",
      "Epoch 21/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.1553\n",
      "Model saved at epoch 21\n",
      "Epoch 22/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.1249\n",
      "Model saved at epoch 22\n",
      "Epoch 23/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.0973\n",
      "Model saved at epoch 23\n",
      "Epoch 24/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.0702\n",
      "Model saved at epoch 24\n",
      "Epoch 25/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.0454\n",
      "Model saved at epoch 25\n",
      "Epoch 26/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.0216\n",
      "Model saved at epoch 26\n",
      "Epoch 27/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.9977\n",
      "Model saved at epoch 27\n",
      "Epoch 28/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.9750\n",
      "Model saved at epoch 28\n",
      "Epoch 29/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.9535\n",
      "Model saved at epoch 29\n",
      "Epoch 30/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.9332\n",
      "Model saved at epoch 30\n",
      "Epoch 31/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.9136\n",
      "Model saved at epoch 31\n",
      "Epoch 32/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.8935\n",
      "Model saved at epoch 32\n",
      "Epoch 33/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.8751\n",
      "Model saved at epoch 33\n",
      "Epoch 34/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.8574\n",
      "Model saved at epoch 34\n",
      "Epoch 35/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.8403\n",
      "Model saved at epoch 35\n",
      "Epoch 36/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.8229\n",
      "Model saved at epoch 36\n",
      "Epoch 37/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.8060\n",
      "Model saved at epoch 37\n",
      "Epoch 38/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.7901\n",
      "Model saved at epoch 38\n",
      "Epoch 39/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.7746\n",
      "Model saved at epoch 39\n",
      "Epoch 40/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.7598\n",
      "Model saved at epoch 40\n",
      "Epoch 41/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.7453\n",
      "Model saved at epoch 41\n",
      "Epoch 42/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.7306\n",
      "Model saved at epoch 42\n",
      "Epoch 43/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.7159\n",
      "Model saved at epoch 43\n",
      "Epoch 44/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.7030\n",
      "Model saved at epoch 44\n",
      "Epoch 45/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6889\n",
      "Model saved at epoch 45\n",
      "Epoch 46/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6761\n",
      "Model saved at epoch 46\n",
      "Epoch 47/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6630\n",
      "Model saved at epoch 47\n",
      "Epoch 48/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6508\n",
      "Model saved at epoch 48\n",
      "Epoch 49/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6379\n",
      "Model saved at epoch 49\n",
      "Epoch 50/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6269\n",
      "Model saved at epoch 50\n",
      "Epoch 51/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6146\n",
      "Model saved at epoch 51\n",
      "Epoch 52/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.6028\n",
      "Model saved at epoch 52\n",
      "Epoch 53/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5920\n",
      "Model saved at epoch 53\n",
      "Epoch 54/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5807\n",
      "Model saved at epoch 54\n",
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5692\n",
      "Model saved at epoch 55\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5580\n",
      "Model saved at epoch 56\n",
      "Epoch 57/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5476\n",
      "Model saved at epoch 57\n",
      "Epoch 58/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5378\n",
      "Model saved at epoch 58\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5277\n",
      "Model saved at epoch 59\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5180\n",
      "Model saved at epoch 60\n",
      "Epoch 61/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.5072\n",
      "Model saved at epoch 61\n",
      "Epoch 62/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4972\n",
      "Model saved at epoch 62\n",
      "Epoch 63/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4878\n",
      "Model saved at epoch 63\n",
      "Epoch 64/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4791\n",
      "Model saved at epoch 64\n",
      "Epoch 65/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4697\n",
      "Model saved at epoch 65\n",
      "Epoch 66/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4600\n",
      "Model saved at epoch 66\n",
      "Epoch 67/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4511\n",
      "Model saved at epoch 67\n",
      "Epoch 68/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4420\n",
      "Model saved at epoch 68\n",
      "Epoch 69/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4332\n",
      "Model saved at epoch 69\n",
      "Epoch 70/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4239\n",
      "Model saved at epoch 70\n",
      "Epoch 71/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4152\n",
      "Model saved at epoch 71\n",
      "Epoch 72/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.4070\n",
      "Model saved at epoch 72\n",
      "Epoch 73/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3989\n",
      "Model saved at epoch 73\n",
      "Epoch 74/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3911\n",
      "Model saved at epoch 74\n",
      "Epoch 75/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3827\n",
      "Model saved at epoch 75\n",
      "Epoch 76/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3750\n",
      "Model saved at epoch 76\n",
      "Epoch 77/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3672\n",
      "Model saved at epoch 77\n",
      "Epoch 78/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3586\n",
      "Model saved at epoch 78\n",
      "Epoch 79/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3506\n",
      "Model saved at epoch 79\n",
      "Epoch 80/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3430\n",
      "Model saved at epoch 80\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3363\n",
      "Model saved at epoch 81\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3281\n",
      "Model saved at epoch 82\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3204\n",
      "Model saved at epoch 83\n",
      "Epoch 84/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3133\n",
      "Model saved at epoch 84\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.3062\n",
      "Model saved at epoch 85\n",
      "Epoch 86/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2991\n",
      "Model saved at epoch 86\n",
      "Epoch 87/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2927\n",
      "Model saved at epoch 87\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2845\n",
      "Model saved at epoch 88\n",
      "Epoch 89/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2774\n",
      "Model saved at epoch 89\n",
      "Epoch 90/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2710\n",
      "Model saved at epoch 90\n",
      "Epoch 91/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2640\n",
      "Model saved at epoch 91\n",
      "Epoch 92/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2571\n",
      "Model saved at epoch 92\n",
      "Epoch 93/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2498\n",
      "Model saved at epoch 93\n",
      "Epoch 94/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2437\n",
      "Model saved at epoch 94\n",
      "Epoch 95/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2373\n",
      "Model saved at epoch 95\n",
      "Epoch 96/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2302\n",
      "Model saved at epoch 96\n",
      "Epoch 97/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2242\n",
      "Model saved at epoch 97\n",
      "Epoch 98/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2173\n",
      "Model saved at epoch 98\n",
      "Epoch 99/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2119\n",
      "Model saved at epoch 99\n",
      "Epoch 100/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.2054\n",
      "Model saved at epoch 100\n",
      "Epoch 101/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1988\n",
      "Model saved at epoch 101\n",
      "Epoch 102/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1930\n",
      "Model saved at epoch 102\n",
      "Epoch 103/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1864\n",
      "Model saved at epoch 103\n",
      "Epoch 104/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1808\n",
      "Model saved at epoch 104\n",
      "Epoch 105/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1741\n",
      "Model saved at epoch 105\n",
      "Epoch 106/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1686\n",
      "Model saved at epoch 106\n",
      "Epoch 107/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1622\n",
      "Model saved at epoch 107\n",
      "Epoch 108/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1563\n",
      "Model saved at epoch 108\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1508\n",
      "Model saved at epoch 109\n",
      "Epoch 110/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1448\n",
      "Model saved at epoch 110\n",
      "Epoch 111/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1394\n",
      "Model saved at epoch 111\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1334\n",
      "Model saved at epoch 112\n",
      "Epoch 113/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1278\n",
      "Model saved at epoch 113\n",
      "Epoch 114/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1217\n",
      "Model saved at epoch 114\n",
      "Epoch 115/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1165\n",
      "Model saved at epoch 115\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1107\n",
      "Model saved at epoch 116\n",
      "Epoch 117/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1054\n",
      "Model saved at epoch 117\n",
      "Epoch 118/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0999\n",
      "Model saved at epoch 118\n",
      "Epoch 119/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0946\n",
      "Model saved at epoch 119\n",
      "Epoch 120/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0891\n",
      "Model saved at epoch 120\n",
      "Epoch 121/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0836\n",
      "Model saved at epoch 121\n",
      "Epoch 122/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0784\n",
      "Model saved at epoch 122\n",
      "Epoch 123/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0739\n",
      "Model saved at epoch 123\n",
      "Epoch 124/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0684\n",
      "Model saved at epoch 124\n",
      "Epoch 125/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0626\n",
      "Model saved at epoch 125\n",
      "Epoch 126/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0572\n",
      "Model saved at epoch 126\n",
      "Epoch 127/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0532\n",
      "Model saved at epoch 127\n",
      "Epoch 128/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0470\n",
      "Model saved at epoch 128\n",
      "Epoch 129/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0424\n",
      "Model saved at epoch 129\n",
      "Epoch 130/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0380\n",
      "Model saved at epoch 130\n",
      "Epoch 131/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0321\n",
      "Model saved at epoch 131\n",
      "Epoch 132/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0283\n",
      "Model saved at epoch 132\n",
      "Epoch 133/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0232\n",
      "Model saved at epoch 133\n",
      "Epoch 134/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0171\n",
      "Model saved at epoch 134\n",
      "Epoch 135/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0122\n",
      "Model saved at epoch 135\n",
      "Epoch 136/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0083\n",
      "Model saved at epoch 136\n",
      "Epoch 137/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.0030\n",
      "Model saved at epoch 137\n",
      "Epoch 138/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9981\n",
      "Model saved at epoch 138\n",
      "Epoch 139/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9939\n",
      "Model saved at epoch 139\n",
      "Epoch 140/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9898\n",
      "Model saved at epoch 140\n",
      "Epoch 141/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9851\n",
      "Model saved at epoch 141\n",
      "Epoch 142/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9803\n",
      "Model saved at epoch 142\n",
      "Epoch 143/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9756\n",
      "Model saved at epoch 143\n",
      "Epoch 144/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9708\n",
      "Model saved at epoch 144\n",
      "Epoch 145/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9671\n",
      "Model saved at epoch 145\n",
      "Epoch 146/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9628\n",
      "Model saved at epoch 146\n",
      "Epoch 147/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9582\n",
      "Model saved at epoch 147\n",
      "Epoch 148/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9530\n",
      "Model saved at epoch 148\n",
      "Epoch 149/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9488\n",
      "Model saved at epoch 149\n",
      "Epoch 150/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [03:03<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.9447\n",
      "Model saved at epoch 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 20/20 [00:42<00:00,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Loss: 3.2214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.221374988555908"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_and_train(tf_generator, conv_data_loader_train, conv_data_loader_test, sp, precision='fp32'):\n",
    "    # Define hyperparameters\n",
    "    num_epochs = 150\n",
    "    learning_rate = 1e-4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Ensure model parameters are in FP32\n",
    "    tf_generator = tf_generator.float().to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "    optimizer = optim.Adam(tf_generator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize the GradScaler for mixed precision training\n",
    "    scaler = GradScaler() if precision == 'fp16' else None\n",
    "\n",
    "    def train_epoch(model, dataloader, optimizer, criterion, device, scaler):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            input_seq = batch[:, :-1]\n",
    "            target = batch[:, 1:]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if precision == 'fp16':\n",
    "                with autocast():\n",
    "                    output = model(input_seq)\n",
    "                    loss = criterion(output.reshape(-1, sp.GetPieceSize()), target.reshape(-1))\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                output = model(input_seq)\n",
    "                loss = criterion(output.reshape(-1, sp.GetPieceSize()), target.reshape(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    def validate(model, dataloader, criterion, device):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "                batch = batch.to(device)\n",
    "                \n",
    "                input_seq = batch[:, :-1]\n",
    "                target = batch[:, 1:]\n",
    "                \n",
    "                if precision == 'fp16':\n",
    "                    with autocast():\n",
    "                        output = model(input_seq)\n",
    "                        loss = criterion(output.reshape(-1, sp.GetPieceSize()), target.reshape(-1))\n",
    "                else:\n",
    "                    output = model(input_seq)\n",
    "                    loss = criterion(output.reshape(-1, sp.GetPieceSize()), target.reshape(-1))\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    # Training loop without validation after every epoch\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss = train_epoch(tf_generator, conv_data_loader_train, optimizer, criterion, device, scaler)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Save the model if the current training loss is better\n",
    "        if train_loss < best_val_loss:\n",
    "            best_val_loss = train_loss\n",
    "            torch.save(tf_generator.state_dict(), f\"best_model_epoch_{epoch+1}_{precision}.pth\")\n",
    "            print(f\"Model saved at epoch {epoch+1}\")\n",
    "\n",
    "    # Perform validation once after training is complete\n",
    "    final_val_loss = validate(tf_generator, conv_data_loader_test, criterion, device)\n",
    "    print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "    return final_val_loss\n",
    "\n",
    "# Usage\n",
    "# Assuming you have already defined and initialized:\n",
    "# tf_generator, conv_data_loader_train, conv_data_loader_test, sp\n",
    "setup_and_train(tf_generator, conv_data_loader_train, conv_data_loader_test, sp, precision='fp16')  # For mixed precision training\n",
    "# setup_and_train(tf_generator, conv_data_loader_train, conv_data_loader_test, sp, precision='fp32')  # For FP32 training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mavg_loss\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Final Training Loss: {avg_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conv_data_loader_train.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved!\n"
     ]
    }
   ],
   "source": [
    "torch.save(tf_generator.state_dict(), 'best_model_2.pth')\n",
    "print(\"New best model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in Mixed Precision Traning vs non Mixed Precision \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NON MIXED PRECISION\n",
    "\n",
    "\n",
    "![alt text](<Screenshot 2024-09-07 130236.png>) \n",
    "\n",
    "\n",
    "MIXED PRECISION\n",
    "\n",
    "![alt text](<Screenshot 2024-09-07 131559.png>) \n",
    "\n",
    "\n",
    " Just look at the difference in \"it/s\" it is much faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "what is apple:  the world of computer programming and web development? it's no secret that software has become a popular choice among tech enthusiasts. with its vast collection, powerful what is apple’t used for various industries such as hardware devices (cms). today we will explore how one particular company can be found in this field using c what is apple tv) technology – specifically focusing on their latest offerings from other digital tools like google cloud services. let us dive right!  **what are you might what is apple video games? **compaces to look\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Nucleus (top-p) sampling\n",
    "def nucleus_sampling(output, p=0.9):\n",
    "    # Sort the logits in descending order and get corresponding indices\n",
    "    sorted_logits, sorted_indices = torch.sort(output, descending=True)\n",
    "    \n",
    "    # Compute cumulative probabilities\n",
    "    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    \n",
    "    # Remove tokens with cumulative probability above the threshold\n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    \n",
    "    # Set the logits of the tokens to remove to a large negative value (effectively removing them)\n",
    "    output[sorted_indices[sorted_indices_to_remove]] = -float(\"Inf\")\n",
    "    \n",
    "    # Sample from the filtered distribution\n",
    "    probabilities = torch.softmax(output, dim=-1)\n",
    "    next_token = torch.multinomial(probabilities, 1).item()\n",
    "    \n",
    "    return next_token\n",
    "\n",
    "# Penalize repetition to reduce repeated words\n",
    "def penalize_repetition(output, generated_tokens, penalty=1.5):\n",
    "    for token in generated_tokens:\n",
    "        output[token] /= penalty\n",
    "    return output\n",
    "\n",
    "# Function to keep generating within the AI topic\n",
    "def reinforce_topic(text, interval=30):\n",
    "    topic_phrase = \" what is apple\"\n",
    "    if len(text) > interval:\n",
    "        text += topic_phrase\n",
    "    return text\n",
    "\n",
    "# Text generation function with nucleus sampling and context reinforcement\n",
    "def generate_text(model, sp, start_text=\"\", max_length=100, temperature=0.7, top_p=0.9, repetition_penalty=1.5, interval=30):\n",
    "    model.eval()\n",
    "    tokens = sp.EncodeAsIds(start_text)\n",
    "    input_seq = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_length):\n",
    "            # Forward pass through the model\n",
    "            output = model(input_seq)\n",
    "            output_logits = output[0, -1, :] / temperature\n",
    "            \n",
    "            # Apply repetition penalty\n",
    "            output_logits = penalize_repetition(output_logits, tokens, penalty=repetition_penalty)\n",
    "            \n",
    "            # Nucleus sampling (top-p)\n",
    "            next_token = nucleus_sampling(output_logits, p=top_p)\n",
    "            \n",
    "            # Append the next token to the sequence\n",
    "            tokens.append(next_token)\n",
    "            input_seq = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Reinforce the topic periodically\n",
    "            if i > 0 and i % interval == 0:\n",
    "                new_text = sp.DecodeIds(tokens)\n",
    "                new_text = reinforce_topic(new_text, interval)\n",
    "                tokens = sp.EncodeAsIds(new_text)\n",
    "                input_seq = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Decode and return the final generated text\n",
    "    return sp.DecodeIds(tokens)\n",
    "\n",
    "# Example of loading the best model\n",
    "tf_generator.load_state_dict(torch.load('best_model_2.pth'))\n",
    "tf_generator = tf_generator.to(device)\n",
    "\n",
    "# Generate text focused on AI\n",
    "sample_text = generate_text(\n",
    "    tf_generator, sp, \n",
    "    start_text=\"what is apple: \", \n",
    "    max_length=100, \n",
    "    temperature=0.2,   # Lower te4mperature for more structured text\n",
    "    top_p=0.4,         # Nucleus sampling with top-p\n",
    "    repetition_penalty=1.5,  # Penalize repetition\n",
    "    interval=30        # Reinforce the topic every 30 tokens\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "print(\"Generated text:\")\n",
    "print(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
